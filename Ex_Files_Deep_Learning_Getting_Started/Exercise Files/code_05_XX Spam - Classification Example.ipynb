{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Setup\n",
    "\n",
    "Install required text processing libraries for the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/huangz36/anaconda3/lib/python3.6/site-packages (3.2.4)\n",
      "Requirement already satisfied: six in /Users/huangz36/anaconda3/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/huangz36/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/huangz36/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/huangz36/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Creating Text Representations\n",
    "\n",
    "Text data needs to be converted to numeric representations before they can be used to train deep learning models. The Spam classification feature data is converted to TF-IDF vectors and the target variable is converted to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Data :\n",
      "------------------------------------\n",
      "  CLASS                                                SMS\n",
      "0   ham   said kiss, kiss, i can't do the sound effects...\n",
      "1   ham      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
      "2  spam  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
      "3  spam  * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
      "4  spam  **FREE MESSAGE**Thanks for using the Auction S...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#Load Spam Data and review content\n",
    "spam_data = pd.read_csv(\"Spam-Classification.csv\")\n",
    "\n",
    "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
    "print(spam_data.head())\n",
    "\n",
    "#Separate feature and target data\n",
    "spam_classes_raw = spam_data[\"CLASS\"]\n",
    "spam_messages = spam_data[\"SMS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape :  (1500, 4640)\n",
      "One-hot Encoding Shape :  (1500, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangz36/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "#Custom tokenizer to remove stopwords and use lemmatization\n",
    "def customtokenize(str):\n",
    "    #Split string as tokens\n",
    "    tokens=nltk.word_tokenize(str)\n",
    "    #Filter for stopwords\n",
    "    nostop = list(filter(lambda token: token not in stopwords.words('english'), tokens))\n",
    "    #Perform lemmatization\n",
    "    lemmatized=[lemmatizer.lemmatize(word) for word in nostop ]\n",
    "    return lemmatized\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Build a TF-IDF Vectorizer model\n",
    "vectorizer = TfidfVectorizer(tokenizer=customtokenize)\n",
    "\n",
    "#Transform feature input to TF-IDF\n",
    "tfidf=vectorizer.fit_transform(spam_messages)\n",
    "#Convert TF-IDF to numpy array\n",
    "tfidf_array = tfidf.toarray()\n",
    "\n",
    "#Build a label encoder for target variable to convert strings to numeric values.\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "spam_classes = label_encoder.fit_transform(\n",
    "                                spam_classes_raw)\n",
    "\n",
    "#Convert target to one-hot encoding vector\n",
    "spam_classes = tf.keras.utils.to_categorical(spam_classes,2)\n",
    "\n",
    "print(\"TF-IDF Matrix Shape : \", tfidf.shape)\n",
    "print(\"One-hot Encoding Shape : \", spam_classes.shape)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split( tfidf_array, spam_classes, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Building and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hidden-Layer-1 (Dense)       (None, 32)                146144    \n",
      "_________________________________________________________________\n",
      "Hidden-Layer-2 (Dense)       (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "Output-Layer (Dense)         (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 147,266\n",
      "Trainable params: 147,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "#Setup Hyper Parameters for building the model\n",
    "NB_CLASSES=2\n",
    "N_HIDDEN=32\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                             input_shape=(X_train.shape[1],),\n",
    "                              name='Hidden-Layer-1',\n",
    "                              activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                              name='Hidden-Layer-2',\n",
    "                              activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                             name='Output-Layer',\n",
    "                             activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Progress:\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c3e866474d49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining Progress:\\n------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m history=model.fit(X_train,\n\u001b[0m\u001b[1;32m     12\u001b[0m           \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Make it verbose so we can see the progress\n",
    "VERBOSE=1\n",
    "\n",
    "#Setup Hyper Parameters for training\n",
    "BATCH_SIZE=256\n",
    "EPOCHS=10\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
    "\n",
    "history=model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "print(\"\\nAccuracy during Training :\\n------------------------------------\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history)[\"accuracy\"].plot(figsize=(8, 5))\n",
    "plt.title(\"Accuracy improvements with Epoch\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Predicting for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangz36/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4640)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ff7642da77ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Predict using model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction Output:\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Predict for multiple samples using batch processing\n",
    "\n",
    "#Convert input into IF-IDF vector using the same vectorizer model\n",
    "predict_tfidf=vectorizer.transform([\"FREE entry to a fun contest\",\n",
    "                                    \"Yup I will come over\"]).toarray()\n",
    "\n",
    "print(predict_tfidf.shape)\n",
    "\n",
    "#Predict using model\n",
    "prediction=np.argmax( model.predict(predict_tfidf), axis=1 )\n",
    "print(\"Prediction Output:\" , prediction)\n",
    "\n",
    "#Print prediction classes\n",
    "print(\"Prediction Classes are \", label_encoder.inverse_transform(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
